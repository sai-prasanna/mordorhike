{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "\n",
    "SEEDS = [13, 19, 42, 94, 1337]\n",
    "STEPS = [100000, 200000, 300000, 400000, 500000]\n",
    "ALGORITHMS = ['Dreamer', 'R2I', 'DRQN']\n",
    "\n",
    "def extract_metrics(metrics_file, metric_name=\"episodes/score_mean\"):\n",
    "    \"\"\"Extract metrics from a JSONL file.\"\"\"\n",
    "    steps_to_values = {}\n",
    "    with metrics_file.open('r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            if metric_name in data:\n",
    "                steps_to_values[data[\"step\"]] = data[metric_name]\n",
    "                \n",
    "    return steps_to_values\n",
    "\n",
    "def collect_algorithm_metrics(algorithm_dir, metric_name=\"episodes/score_mean\"):\n",
    "    \"\"\"Collect metrics for all seeds of an algorithm.\"\"\"\n",
    "    all_seed_metrics = []\n",
    "    \n",
    "    # Find all seed directories\n",
    "    seed_dirs = [algorithm_dir / f\"{seed}\" for seed in SEEDS]\n",
    "    \n",
    "    for seed_dir in seed_dirs:\n",
    "        metrics_file = seed_dir / \"eval\" / \"metrics.jsonl\"\n",
    "        if not metrics_file.exists():\n",
    "            print(f\"Metrics file {metrics_file} does not exist\")\n",
    "            continue\n",
    "        steps_to_values = extract_metrics(metrics_file, metric_name)    \n",
    "        all_seed_metrics.append([steps_to_values[step] for step in STEPS])\n",
    "    # return array of seeds x steps\n",
    "    return np.array(all_seed_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find all algorithm directories\n",
    "# algo_dirs = [\n",
    "    \n",
    "# ]\n",
    "\n",
    "# all_algorithms_data = {}\n",
    "\n",
    "# for algo_dir in algo_dirs:\n",
    "#     algo_name, metrics = collect_algorithm_metrics(algo_dir, args.metric)\n",
    "#     all_algorithms_data[algo_name] = metrics\n",
    "\n",
    "# plot_algorithms_comparison(all_algorithms_data, args.metric, args.output)\n",
    "\n",
    "# print(f\"Processed {len(all_algorithms_data)} algorithms:\")\n",
    "# for algo in all_algorithms_data:\n",
    "#     print(f\"  - {algo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics file /work/dlclarge1/ramans-powm/powm/experiments/mordor_hike/046_r2i_medium_tuned/42/eval/metrics.jsonl does not exist\n",
      "Metrics file /work/dlclarge1/ramans-powm/powm/experiments/mordor_hike/046_r2i_medium_tuned/94/eval/metrics.jsonl does not exist\n",
      "Metrics file /work/dlclarge1/ramans-powm/powm/experiments/mordor_hike/046_r2i_medium_tuned/42/eval/metrics.jsonl does not exist\n",
      "Metrics file /work/dlclarge1/ramans-powm/powm/experiments/mordor_hike/046_r2i_medium_tuned/94/eval/metrics.jsonl does not exist\n",
      "Metrics file /work/dlclarge1/ramans-powm/powm/experiments/mordor_hike/046_r2i_medium_tuned/42/eval/metrics.jsonl does not exist\n",
      "Metrics file /work/dlclarge1/ramans-powm/powm/experiments/mordor_hike/046_r2i_medium_tuned/94/eval/metrics.jsonl does not exist\n",
      "Metrics file /work/dlclarge1/ramans-powm/powm/experiments/mordor_hike/046_r2i_medium_tuned/42/eval/metrics.jsonl does not exist\n",
      "Metrics file /work/dlclarge1/ramans-powm/powm/experiments/mordor_hike/046_r2i_medium_tuned/94/eval/metrics.jsonl does not exist\n",
      "Metrics file /work/dlclarge1/ramans-powm/powm/experiments/mordor_hike/046_r2i_medium_tuned/42/eval/metrics.jsonl does not exist\n",
      "Metrics file /work/dlclarge1/ramans-powm/powm/experiments/mordor_hike/046_r2i_medium_tuned/94/eval/metrics.jsonl does not exist\n",
      "Metrics file /work/dlclarge1/ramans-powm/powm/experiments/mordor_hike/046_r2i_medium_tuned/42/eval/metrics.jsonl does not exist\n",
      "Metrics file /work/dlclarge1/ramans-powm/powm/experiments/mordor_hike/046_r2i_medium_tuned/94/eval/metrics.jsonl does not exist\n",
      "Metrics file /work/dlclarge1/ramans-powm/powm/experiments/mordor_hike/046_r2i_medium_tuned/42/eval/metrics.jsonl does not exist\n",
      "Metrics file /work/dlclarge1/ramans-powm/powm/experiments/mordor_hike/046_r2i_medium_tuned/94/eval/metrics.jsonl does not exist\n",
      "Metrics file /work/dlclarge1/ramans-powm/powm/experiments/mordor_hike/046_r2i_medium_tuned/42/eval/metrics.jsonl does not exist\n",
      "Metrics file /work/dlclarge1/ramans-powm/powm/experiments/mordor_hike/046_r2i_medium_tuned/94/eval/metrics.jsonl does not exist\n",
      "Metrics file /work/dlclarge1/ramans-powm/powm/experiments/mordor_hike/046_r2i_medium_tuned/42/eval/metrics.jsonl does not exist\n",
      "Metrics file /work/dlclarge1/ramans-powm/powm/experiments/mordor_hike/046_r2i_medium_tuned/94/eval/metrics.jsonl does not exist\n",
      "Metrics file /work/dlclarge1/ramans-powm/powm/experiments/mordor_hike/046_r2i_medium_tuned/42/eval/metrics.jsonl does not exist\n",
      "Metrics file /work/dlclarge1/ramans-powm/powm/experiments/mordor_hike/046_r2i_medium_tuned/94/eval/metrics.jsonl does not exist\n",
      "Metrics file /work/dlclarge1/ramans-powm/powm/experiments/mordor_hike/046_r2i_medium_tuned/42/eval/metrics.jsonl does not exist\n",
      "Metrics file /work/dlclarge1/ramans-powm/powm/experiments/mordor_hike/046_r2i_medium_tuned/94/eval/metrics.jsonl does not exist\n",
      "Metrics file /work/dlclarge1/ramans-powm/powm/experiments/mordor_hike/046_r2i_medium_tuned/42/eval/metrics.jsonl does not exist\n",
      "Metrics file /work/dlclarge1/ramans-powm/powm/experiments/mordor_hike/046_r2i_medium_tuned/94/eval/metrics.jsonl does not exist\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "from rliable import metrics\n",
    "from rliable import library as rly\n",
    "from rliable import plot_utils\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scores_2_iqm = lambda scores: np.array([metrics.aggregate_iqm(scores[..., env_step_idx])\n",
    "                               for env_step_idx in range(scores.shape[-1])])\n",
    "\n",
    "\n",
    "base_dir = pathlib.Path('/work/dlclarge1/ramans-powm/powm/experiments/mordor_hike')\n",
    "\n",
    "env2algos = {\n",
    "    'easy': {\n",
    "        'Dreamer': base_dir / '044_dreamer_easy_tuned',\n",
    "        'R2I': base_dir / '046_r2i_easy_tuned',\n",
    "        'DRQN': base_dir / '045_drqn_easy_tuned',\n",
    "    },\n",
    "    'medium': {\n",
    "        'Dreamer': base_dir / '044_dreamer_medium_tuned',\n",
    "        'R2I': base_dir / '046_r2i_medium_tuned',\n",
    "        'DRQN': base_dir / '045_drqn_medium_tuned',\n",
    "    },\n",
    "    'hard': {\n",
    "        'Dreamer': base_dir / '044_dreamer_hard_tuned',\n",
    "        'R2I': base_dir / '046_r2i_hard_tuned',\n",
    "        'DRQN': base_dir / '045_drqn_hard_tuned',\n",
    "    },\n",
    "}\n",
    "\n",
    "save_path = pathlib.Path('metrics')\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metric_meta = {\n",
    "    'score_mean': {\n",
    "        'ylabel': 'Score (IQM)',\n",
    "        'file_name': 'iqm_scores',\n",
    "    },\n",
    "    'episodic_kldiv': {\n",
    "        'ylabel': 'KL Divergence',\n",
    "        'file_name': 'episodic_kldiv',\n",
    "    },\n",
    "    'score_episodic_kldiv_corr_pearson': {\n",
    "        'ylabel': 'Pearson\\'s r',\n",
    "        'file_name': 'pearson_r',\n",
    "    },\n",
    "    'score_episodic_kldiv_corr_spearman': {\n",
    "        'ylabel': \"Spearman's r\",\n",
    "        'file_name': 'spearman_r',\n",
    "    },\n",
    "}\n",
    "\n",
    "trajectory_meta = {\n",
    "    'episodes': 'in_distribution',\n",
    "    'noisy_episodes': 'noisy_ood',\n",
    "    'waypoint_episodes': 'waypoint_ood',\n",
    "}\n",
    "\n",
    "\n",
    "# for trajectory_type, trajectory_name in trajectory_meta.items():\n",
    "#     for metric, metric_info in metric_meta.items():\n",
    "#         for env in ['easy', 'medium', 'hard']:\n",
    "#             algo_scores = {}\n",
    "#             for algo, path in env2algos[env].items():\n",
    "#                 algo_scores[algo] = collect_algorithm_metrics(pathlib.Path(path),f'{trajectory_type}/{metric}')\n",
    "#             iqm_scores, iqm_cis = rly.get_interval_estimates(\n",
    "#             algo_scores, scores_2_iqm, reps=50000)\n",
    "#             fig, ax = plt.subplots(figsize=(7, 5))\n",
    "#             plot_utils.plot_sample_efficiency_curve(\n",
    "#                 [f\"{step / 1000:.0f}k\" for step in STEPS], iqm_scores, iqm_cis, algorithms=ALGORITHMS,\n",
    "#                 ax=ax,\n",
    "#                 xlabel=r'Steps',\n",
    "#                 ylabel=metric_info['ylabel'], legend=True)\n",
    "#             fig.savefig(save_path / f'{env}_{metric_info[\"file_name\"]}_{trajectory_name}.pdf', dpi=300, bbox_inches='tight')\n",
    "for trajectory_type, trajectory_name in trajectory_meta.items():\n",
    "    for metric, metric_info in metric_meta.items():\n",
    "        # Create one figure with 3 subplots for easy, medium, hard\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=True)\n",
    "        \n",
    "        # Collect data for all environments first\n",
    "        all_env_data = {}\n",
    "        for i, env in enumerate(['easy', 'medium', 'hard']):\n",
    "            algo_scores = {}\n",
    "            for algo, path in env2algos[env].items():\n",
    "                algo_scores[algo] = collect_algorithm_metrics(pathlib.Path(path),f'{trajectory_type}/{metric}')\n",
    "            \n",
    "            iqm_scores, iqm_cis = rly.get_interval_estimates(\n",
    "                algo_scores, scores_2_iqm, reps=50000)\n",
    "            \n",
    "            all_env_data[env] = {\n",
    "                'iqm_scores': iqm_scores,\n",
    "                'iqm_cis': iqm_cis\n",
    "            }\n",
    "            \n",
    "            # Plot on the respective subplot\n",
    "            plot_utils.plot_sample_efficiency_curve(\n",
    "                [f\"{step / 1000:.0f}k\" for step in STEPS], \n",
    "                iqm_scores, iqm_cis, \n",
    "                algorithms=ALGORITHMS,\n",
    "                ax=axes[i],\n",
    "                xlabel=r'Steps',\n",
    "                ylabel=metric_info['ylabel'] if i == 0 else '',  # Only show y-label on first subplot\n",
    "                legend=False)  # No legend for individual subplots\n",
    "            \n",
    "            # Set title for each subplot\n",
    "            axes[i].set_title(f'{env.capitalize()}')\n",
    "        \n",
    "        # Add a single legend for the entire figure\n",
    "        handles, labels = axes[0].get_legend_handles_labels()\n",
    "        fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.12), ncol=len(ALGORITHMS))\n",
    "        \n",
    "        # Adjust layout and save\n",
    "        plt.tight_layout()\n",
    "        fig.subplots_adjust(bottom=0.2)  # Make room for the legend\n",
    "        fig.savefig(save_path / f'{metric_info[\"file_name\"]}_{trajectory_name}.pdf', dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
