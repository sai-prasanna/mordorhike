{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "\n",
    "SEEDS = [13, 19, 42, 94, 1337]\n",
    "STEPS = [100000, 200000, 300000, 400000, 500000]\n",
    "ALGORITHMS = ['Dreamer', 'R2I', 'DRQN']\n",
    "\n",
    "def extract_metrics(metrics_file, metric_name=\"episodes/score_mean\"):\n",
    "    \"\"\"Extract metrics from a JSONL file.\"\"\"\n",
    "    steps_to_values = {}\n",
    "    with metrics_file.open('r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            if metric_name in data:\n",
    "                steps_to_values[data[\"step\"]] = data[metric_name]\n",
    "                \n",
    "    return steps_to_values\n",
    "\n",
    "def collect_algorithm_metrics(algorithm_dir, eval_dir, metric_name=\"episodes/score_mean\"):\n",
    "    \"\"\"Collect metrics for all seeds of an algorithm.\"\"\"\n",
    "    all_seed_metrics = []\n",
    "    \n",
    "    # Find all seed directories\n",
    "    seed_dirs = [algorithm_dir / f\"{seed}\" for seed in SEEDS]\n",
    "    \n",
    "    for seed_dir in seed_dirs:\n",
    "        metrics_file = seed_dir / eval_dir / \"metrics.jsonl\"\n",
    "        if not metrics_file.exists():\n",
    "            print(f\"Metrics file {metrics_file} does not exist\")\n",
    "            continue\n",
    "        steps_to_values = extract_metrics(metrics_file, metric_name)    \n",
    "        all_seed_metrics.append([steps_to_values[step] for step in STEPS])\n",
    "    # return array of seeds x steps\n",
    "    return np.array(all_seed_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from rliable import metrics\n",
    "from rliable import library as rly\n",
    "from rliable import plot_utils\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scores_2_iqm = lambda scores: np.array([metrics.aggregate_iqm(scores[..., env_step_idx])\n",
    "                               for env_step_idx in range(scores.shape[-1])])\n",
    "\n",
    "\n",
    "base_dir = pathlib.Path('/work/dlclarge1/ramans-powm/powm/experiments/mordor_hike')\n",
    "\n",
    "env2algos = {\n",
    "    'easy': {\n",
    "        'Dreamer': base_dir / '044_dreamer_easy_tuned',\n",
    "        'R2I': base_dir / '046_r2i_easy_tuned',\n",
    "        'DRQN': base_dir / '045_drqn_easy_tuned',\n",
    "    },\n",
    "    'medium': {\n",
    "        'Dreamer': base_dir / '044_dreamer_medium_tuned',\n",
    "        'R2I': base_dir / '046_r2i_medium_tuned',\n",
    "        'DRQN': base_dir / '045_drqn_medium_tuned',\n",
    "    },\n",
    "    'hard': {\n",
    "        'Dreamer': base_dir / '044_dreamer_hard_tuned',\n",
    "        'R2I': base_dir / '046_r2i_hard_tuned',\n",
    "        'DRQN': base_dir / '045_drqn_hard_tuned',\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "metric_meta = {\n",
    "    'score_mean': {\n",
    "        'ylabel': 'Average Score',\n",
    "        'file_name': 'iqm_scores',\n",
    "    },\n",
    "    'episodic_kldiv': {\n",
    "        'ylabel': 'Average KL Divergence',\n",
    "        'file_name': 'episodic_kldiv',\n",
    "    },\n",
    "    'success_mean': {\n",
    "        'ylabel': 'Success Rate',\n",
    "        'file_name': 'success_mean',\n",
    "    },\n",
    "}\n",
    "\n",
    "trajectory_meta = {\n",
    "    'episodes': 'in_distribution',\n",
    "    'noisy_episodes': 'noisy_ood',\n",
    "    'waypoint_episodes': 'waypoint_ood',\n",
    "}\n",
    "\n",
    "eval_dir = 'eval_with_angle'\n",
    "# Initialize a nested dictionary to store all plotted data for tables\n",
    "plot_data = defaultdict(lambda: defaultdict(dict))\n",
    "save_path = pathlib.Path(eval_dir)\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Add dictionaries to store global min and max values for each metric\n",
    "metric_global_min = defaultdict(lambda: float('inf'))\n",
    "metric_global_max = defaultdict(lambda: float('-inf'))\n",
    "\n",
    "# Loop through each trajectory type and metric to create plots and store data\n",
    "for trajectory_type, trajectory_name in trajectory_meta.items():\n",
    "    for metric, metric_info in metric_meta.items():\n",
    "        # Collect data for all environments\n",
    "        for i, env in enumerate(['easy', 'medium', 'hard']):\n",
    "            algo_scores = {}\n",
    "            for algo, path in env2algos[env].items():\n",
    "                algo_scores[algo] = collect_algorithm_metrics(pathlib.Path(path), eval_dir, f'{trajectory_type}/{metric}')\n",
    "            \n",
    "            iqm_scores, iqm_cis = rly.get_interval_estimates(\n",
    "                algo_scores, scores_2_iqm, reps=50000)\n",
    "            \n",
    "            # Update global min and max for this metric\n",
    "            for algo in ALGORITHMS:\n",
    "                metric_global_min[metric] = min(metric_global_min[metric], np.min(iqm_scores[algo]))\n",
    "                metric_global_max[metric] = max(metric_global_max[metric], np.max(iqm_scores[algo]))\n",
    "            \n",
    "            # Store the data for tables\n",
    "            plot_data[metric][env][trajectory_type] = {\n",
    "                'iqm_scores': iqm_scores,\n",
    "                'iqm_cis': iqm_cis\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trajectory_type, trajectory_name in trajectory_meta.items():\n",
    "    for metric, metric_info in metric_meta.items():\n",
    "        # Create one figure with 3 subplots for easy, medium, hard\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=True)\n",
    "        \n",
    "        # Apply consistent y-limits across all subplots using global min/max\n",
    "        y_min = metric_global_min[metric]\n",
    "        y_max = metric_global_max[metric]\n",
    "        # Add a small padding (5%) to prevent curves from touching the edges\n",
    "        padding = abs((y_max - y_min) * 0.05)\n",
    "        y_min -= padding\n",
    "        y_max += padding\n",
    "        \n",
    "        for i, env in enumerate(['easy', 'medium', 'hard']):\n",
    "            # Get data for this environment\n",
    "            iqm_scores = plot_data[metric][env][trajectory_type]['iqm_scores']\n",
    "            iqm_cis = plot_data[metric][env][trajectory_type]['iqm_cis']\n",
    "            \n",
    "            # Plot on the respective subplot\n",
    "            plot_utils.plot_sample_efficiency_curve(\n",
    "                [f\"{step / 1000:.0f}k\" for step in STEPS], \n",
    "                iqm_scores, iqm_cis, \n",
    "                algorithms=ALGORITHMS,\n",
    "                ax=axes[i],\n",
    "                xlabel=r'Steps' if i == 1 else '', # show x-label only for second subplot\n",
    "                ylabel=metric_info['ylabel'] if i == 0 else '',  # Only show y-label on first subplot\n",
    "                legend=False)  # No legend for individual subplots\n",
    "            \n",
    "            # Set consistent y-limits\n",
    "            axes[i].set_ylim(y_min, y_max)\n",
    "            # Increase font size for x and y labels\n",
    "            if i == 1:\n",
    "                axes[i].xaxis.label.set_fontsize(18)  # Larger x-label font\n",
    "            if i == 0:\n",
    "                axes[i].yaxis.label.set_fontsize(18)  # Larger y-label font\n",
    "            \n",
    "            # Set title for each subplot\n",
    "            axes[i].set_title(f'{env.capitalize()}', fontsize=18)\n",
    "        \n",
    "        # Add a single legend for the entire figure\n",
    "        handles, labels = axes[0].get_legend_handles_labels()\n",
    "        fig.legend(handles, labels, loc='lower right', ncol=len(ALGORITHMS), fontsize=18)\n",
    "        \n",
    "        # Adjust layout and save\n",
    "        plt.tight_layout()\n",
    "        fig.subplots_adjust(bottom=0.2)  # Make room for the legend\n",
    "        fig.savefig(save_path / f'{metric_info[\"file_name\"]}_{trajectory_name}.pdf', dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 138\u001b[0m\n\u001b[1;32m    133\u001b[0m             f\u001b[38;5;241m.\u001b[39mwrite(table)\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated consolidated LaTeX table for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 138\u001b[0m create_consolidated_latex_tables(\u001b[43mplot_data\u001b[49m, save_path, \u001b[38;5;28;01mNone\u001b[39;00m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnoisy_episodes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwaypoint_episodes\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ood\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    140\u001b[0m create_consolidated_latex_tables(plot_data, save_path, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuccess_mean\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_data' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def create_consolidated_latex_tables(plot_data, save_path, metrics_to_include=None, traj_types_to_include=None, file_suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Create tables for specified metrics where each table shows algorithm performance\n",
    "    across specified trajectory types and difficulty levels.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    plot_data: The data dictionary containing all metrics\n",
    "    save_path: Path to save the generated tables\n",
    "    metrics_to_include: List of metrics to include (if None, includes all metrics)\n",
    "    traj_types_to_include: List of trajectory types to include (if None, includes all types)\n",
    "    file_suffix: Optional suffix to add to filenames\n",
    "    \"\"\"\n",
    "    \n",
    "    all_metrics_table = {\n",
    "        'score_mean': {\n",
    "            'caption': 'Final average score comparison (IQM across seeds) across different evaluation conditions and difficulty levels. Agent is evaluated at the 500k steps. Higher values are better.',\n",
    "            'label': 'tab:final_scores_consolidated',\n",
    "            'filename': 'final_scores_consolidated',\n",
    "            'higher_is_better': True\n",
    "        },\n",
    "        'episodic_kldiv': {\n",
    "            'caption': 'Final KL divergence comparison (IQM across seeds) across different evaluation conditions and difficulty levels. Agent is evaluated at the 500k steps. Lower values indicate better state estimation.',\n",
    "            'label': 'tab:final_kldiv_consolidated',\n",
    "            'filename': 'final_kldiv_consolidated',\n",
    "            'higher_is_better': False\n",
    "        },\n",
    "        'success_mean': {\n",
    "            'caption': 'Final average success rate comparison (IQM across seeds) across different evaluation conditions and difficulty levels. Agent is evaluated at the 500k steps. Higher values are better.',\n",
    "            'label': 'tab:final_success_consolidated',\n",
    "            'filename': 'final_success_consolidated',\n",
    "            'higher_is_better': True\n",
    "        }\n",
    "    }\n",
    "    if metrics_to_include is not None:\n",
    "        all_metrics_table = {k: v for k, v in all_metrics_table.items() if k in metrics_to_include}\n",
    "        \n",
    "    \n",
    "    # Default trajectory types if not specified\n",
    "    if traj_types_to_include is None:\n",
    "        traj_types_to_include = ['episodes', 'noisy_episodes', 'waypoint_episodes']\n",
    "    \n",
    "    traj_display_names = {\n",
    "        'episodes': 'In-distribution',\n",
    "        'noisy_episodes': 'Action noise OOD',\n",
    "        'waypoint_episodes': 'Waypoint OOD'\n",
    "    }\n",
    "    \n",
    "    # Final step index (500000 should be the last step)\n",
    "    final_step_idx = -1  # Last step\n",
    "    \n",
    "    # Create one table per metric\n",
    "    for metric, meta in all_metrics_table.items():\n",
    "        # Create the table header\n",
    "        table = \"\\\\begin{table}[ht]\\n\\\\centering\\n\\\\resizebox{\\\\textwidth}{!}{\\n\"\n",
    "        \n",
    "        # Determine number of columns based on trajectory types\n",
    "        num_traj_types = len(traj_types_to_include)\n",
    "        num_cols_per_traj = 3  # easy, medium, hard\n",
    "        total_cols = num_traj_types * num_cols_per_traj\n",
    "        \n",
    "        # Create tabular with appropriate number of columns\n",
    "        table += f\"\\\\begin{{tabular}}{{l|{('c' * num_cols_per_traj + '|') * (num_traj_types - 1) + 'c' * num_cols_per_traj}}}\\n\\\\toprule\\n\"\n",
    "        \n",
    "        # Add trajectory type headers with subcolumns for difficulty levels\n",
    "        table += \"& \"\n",
    "        for i, traj_type in enumerate(traj_types_to_include):\n",
    "            is_last = i == len(traj_types_to_include) - 1\n",
    "            border = \"\" if is_last else \"|\"\n",
    "            table += f\"\\\\multicolumn{{3}}{{c{border}}}{{\\\\textbf{{{traj_display_names[traj_type]}}}}} & \" if not is_last else f\"\\\\multicolumn{{3}}{{c}}{{\\\\textbf{{{traj_display_names[traj_type]}}}}} \"\n",
    "        table += \"\\\\\\\\\\n\"\n",
    "        \n",
    "        # Add difficulty level subheaders\n",
    "        table += \"\\\\textbf{Algorithm} \"\n",
    "        for _ in range(num_traj_types):  # For each trajectory type\n",
    "            table += \"& \\\\textbf{Easy} & \\\\textbf{Medium} & \\\\textbf{Hard} \"\n",
    "        table += \"\\\\\\\\\\n\\\\midrule\\n\"\n",
    "        \n",
    "        # Find the best value for each column\n",
    "        best_values = {}\n",
    "        for traj_type in traj_types_to_include:\n",
    "            for env in ['easy', 'medium', 'hard']:\n",
    "                column_values = []\n",
    "                for algo in ALGORITHMS:\n",
    "                    data = plot_data[metric][env][traj_type]\n",
    "                    final_value = data['iqm_scores'][algo][final_step_idx]\n",
    "                    column_values.append(final_value)\n",
    "                \n",
    "                if meta['higher_is_better']:\n",
    "                    best_values[(traj_type, env)] = max(column_values)\n",
    "                else:\n",
    "                    best_values[(traj_type, env)] = min(column_values)\n",
    "        \n",
    "        # Add data rows\n",
    "        for algo in ALGORITHMS:\n",
    "            table += f\"{algo} \"\n",
    "            for traj_type in traj_types_to_include:\n",
    "                for env in ['easy', 'medium', 'hard']:\n",
    "                    data = plot_data[metric][env][traj_type]\n",
    "                    final_value = data['iqm_scores'][algo][final_step_idx]\n",
    "                    \n",
    "                    # Check if this is the best value for this column\n",
    "                    is_best = abs(final_value - best_values[(traj_type, env)]) < 1e-6\n",
    "                    \n",
    "                    # Format with appropriate precision and highlight if best\n",
    "                    if metric == 'success_mean':\n",
    "                        # Format as percentage\n",
    "                        formatted_value = f\"{final_value*100:.1f}\\\\%\"\n",
    "                    elif metric == 'episodic_kldiv':\n",
    "                        # Format with more decimals for small values\n",
    "                        formatted_value = f\"{final_value:.3f}\"\n",
    "                    else:\n",
    "                        # Standard format for scores\n",
    "                        formatted_value = f\"{final_value:.2f}\"\n",
    "                    \n",
    "                    if is_best:\n",
    "                        table += f\"& \\\\textbf{{{formatted_value}}} \"\n",
    "                    else:\n",
    "                        table += f\"& {formatted_value} \"\n",
    "            table += \"\\\\\\\\\\n\"\n",
    "        \n",
    "        # Finish the table\n",
    "        table += \"\\\\bottomrule\\n\\\\end{tabular}\\n}\\n\"\n",
    "        table += f\"\\\\caption{{{meta['caption']}}}\\n\"\n",
    "        table += f\"\\\\label{{{meta['label']}_{file_suffix}}}\\n\"\n",
    "        table += \"\\\\end{table}\"\n",
    "        \n",
    "        # Add suffix to filename\n",
    "        filename = f\"{meta['filename']}{file_suffix}.tex\" if file_suffix else f\"{meta['filename']}.tex\"\n",
    "        \n",
    "        # Save the table to file\n",
    "        with open(save_path / filename, 'w') as f:\n",
    "            f.write(table)\n",
    "        \n",
    "        print(f\"Created consolidated LaTeX table for {metric} at {save_path / filename}\")\n",
    "\n",
    "\n",
    "create_consolidated_latex_tables(plot_data, save_path, None, ['noisy_episodes', 'waypoint_episodes'], \"_ood\")\n",
    "\n",
    "create_consolidated_latex_tables(plot_data, save_path, ['success_mean'], None, \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created LaTeX table for score_mean (In-distribution) at eval_with_angle/final_scores_in_distribution.tex\n",
      "Created LaTeX table for episodic_kldiv (In-distribution) at eval_with_angle/final_kldiv_in_distribution.tex\n",
      "Created LaTeX table for success_mean (In-distribution) at eval_with_angle/final_success_in_distribution.tex\n",
      "Created LaTeX table for score_mean (Action noise OOD) at eval_with_angle/final_scores_action_noise_ood.tex\n",
      "Created LaTeX table for episodic_kldiv (Action noise OOD) at eval_with_angle/final_kldiv_action_noise_ood.tex\n",
      "Created LaTeX table for success_mean (Action noise OOD) at eval_with_angle/final_success_action_noise_ood.tex\n",
      "Created LaTeX table for score_mean (Waypoint OOD) at eval_with_angle/final_scores_waypoint_ood.tex\n",
      "Created LaTeX table for episodic_kldiv (Waypoint OOD) at eval_with_angle/final_kldiv_waypoint_ood.tex\n",
      "Created LaTeX table for success_mean (Waypoint OOD) at eval_with_angle/final_success_waypoint_ood.tex\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metrics_to_table = {\n",
    "    'score_mean': {\n",
    "        'caption_template': '\\\\textbf{{{}: Average score}} comparison across difficulty levels. We report the IQM across seeds. Agent is evaluated at the 500k steps. Higher values are better.',\n",
    "        'label_template': 'tab:final_scores_{}',\n",
    "        'filename_template': 'final_scores_{}.tex',\n",
    "        'higher_is_better': True\n",
    "    },\n",
    "    'episodic_kldiv': {\n",
    "        'caption_template': '\\\\textbf{{{}: Average KL divergence}} comparison across difficulty levels. We report the IQM across seeds. Agent is evaluated at the 500k steps. Lower values indicate better state estimation.',\n",
    "        'label_template': 'tab:final_kldiv_{}',\n",
    "        'filename_template': 'final_kldiv_{}.tex',\n",
    "        'higher_is_better': False\n",
    "    },\n",
    "    'success_mean': {\n",
    "        'caption_template': '\\\\textbf{{{}: Success rate}} comparison across difficulty levels. We report the IQM across seeds. Agent is evaluated at the 500k steps. Higher values are better.',\n",
    "        'label_template': 'tab:final_success_{}',\n",
    "        'filename_template': 'final_success_{}.tex',\n",
    "        'higher_is_better': True\n",
    "    }\n",
    "}\n",
    "\n",
    "traj_display_names = {\n",
    "    'episodes': 'In-distribution',\n",
    "    'noisy_episodes': 'Action noise OOD',\n",
    "    'waypoint_episodes': 'Waypoint OOD'\n",
    "}\n",
    "trajectory_label = {\n",
    "    'episodes': 'in_distribution',\n",
    "    'noisy_episodes': 'action_noise_ood',\n",
    "    'waypoint_episodes': 'waypoint_ood'\n",
    "}\n",
    "\n",
    "def create_separate_latex_tables(plot_data, save_path):\n",
    "    \"\"\"\n",
    "    Create separate tables for each trajectory type (in-distribution, action noise OOD, waypoint OOD).\n",
    "    For each trajectory type, create 3 tables (one per metric).\n",
    "    Highlights the best performance in each column with bold text.\n",
    "    \"\"\"\n",
    "    # Final step index (500000 should be the last step)\n",
    "    final_step_idx = -1  # Last step\n",
    "    \n",
    "    # Create tables for each trajectory type\n",
    "    for traj_type, traj_name in traj_display_names.items():\n",
    "        # Create one table per metric for this trajectory type\n",
    "        for metric, meta in metrics_to_table.items():\n",
    "            # Fill in the templates with trajectory name\n",
    "            caption = meta['caption_template'].format(traj_name)\n",
    "            traj_label = trajectory_label[traj_type]\n",
    "\n",
    "            label = meta['label_template'].format(traj_label)\n",
    "            filename = meta['filename_template'].format(traj_label)\n",
    "            \n",
    "            # Create the table header\n",
    "            table = \"\\\\begin{table}[ht]\\n\\\\centering\\n\"\n",
    "            table += \"\\\\begin{tabular}{l|ccc}\\n\\\\toprule\\n\"\n",
    "            \n",
    "            # Add difficulty level headers\n",
    "            table += \"\\\\textbf{Algorithm} & \\\\textbf{Easy} & \\\\textbf{Medium} & \\\\textbf{Hard} \\\\\\\\\\n\\\\midrule\\n\"\n",
    "            \n",
    "            # Find the best value for each column (difficulty level)\n",
    "            best_values = {}\n",
    "            for env in ['easy', 'medium', 'hard']:\n",
    "                column_values = []\n",
    "                for algo in ALGORITHMS:\n",
    "                    data = plot_data[metric][env][traj_type]\n",
    "                    final_value = data['iqm_scores'][algo][final_step_idx]\n",
    "                    column_values.append(final_value)\n",
    "                \n",
    "                if meta['higher_is_better']:\n",
    "                    best_values[env] = max(column_values)\n",
    "                else:\n",
    "                    best_values[env] = min(column_values)\n",
    "            \n",
    "            # Add data rows\n",
    "            for algo in ALGORITHMS:\n",
    "                table += f\"{algo} \"\n",
    "                for env in ['easy', 'medium', 'hard']:\n",
    "                    data = plot_data[metric][env][traj_type]\n",
    "                    final_value = data['iqm_scores'][algo][final_step_idx]\n",
    "                    \n",
    "                    # Check if this is the best value for this column\n",
    "                    is_best = abs(final_value - best_values[env]) < 1e-6\n",
    "                    \n",
    "                    # Format with appropriate precision and highlight if best\n",
    "                    if metric == 'success_mean':\n",
    "                        # Format as percentage\n",
    "                        formatted_value = f\"{final_value*100:.1f}\\\\%\"\n",
    "                    elif metric == 'episodic_kldiv':\n",
    "                        # Format with more decimals for small values\n",
    "                        formatted_value = f\"{final_value:.3f}\"\n",
    "                    else:\n",
    "                        # Standard format for scores\n",
    "                        formatted_value = f\"{final_value:.2f}\"\n",
    "                    \n",
    "                    if is_best:\n",
    "                        table += f\"& \\\\textbf{{{formatted_value}}} \"\n",
    "                    else:\n",
    "                        table += f\"& {formatted_value} \"\n",
    "                table += \"\\\\\\\\\\n\"\n",
    "            \n",
    "            # Finish the table\n",
    "            table += \"\\\\bottomrule\\n\\\\end{tabular}\\n\"\n",
    "            table += f\"\\\\caption{{{caption}}}\\n\"\n",
    "            table += f\"\\\\label{{{label}}}\\n\"\n",
    "            table += \"\\\\end{table}\"\n",
    "            \n",
    "            \n",
    "            # Save the table to file\n",
    "            with open(save_path / filename, 'w') as f:\n",
    "                f.write(table)\n",
    "            \n",
    "            print(f\"Created LaTeX table for {metric} ({traj_name}) at {save_path / filename}\")\n",
    "\n",
    "# Create separate tables for each trajectory type\n",
    "create_separate_latex_tables(plot_data, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
